{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIVIATHON\n",
    "\n",
    "## Team Member:\n",
    "### Adil Hamid Malla,Sumeet Singh Arora, Rahul Bhagat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Problem Statement\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Work\n",
    "\n",
    "TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Plan & Methodology\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping Using Pywikibot\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User Configuration for pywikibot\n",
    "family = 'wikipedia'\n",
    "mylang = 'en'\n",
    "usernames['wikipedia']['en'] = u'ExampleBot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Parser\n",
    "\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wiki Parser Class\n",
    "import pywikibot\n",
    "import wiki2plain\n",
    "import re\n",
    "import random\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        print(\"Making the Instance of Wiki parser.\")\n",
    "        self.site = pywikibot.Site('en', 'wikipedia')\n",
    "        self.cache_stem = {}\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.k = 50\n",
    "\n",
    "    def getEntityTokens(self, wiki_entity):\n",
    "        site = pywikibot.Site('en', 'wikipedia')\n",
    "        page = pywikibot.Page(site, wiki_entity)  #here we just crawl for the new entry\n",
    "        text = page.text\n",
    "        wiki2plain_instance = wiki2plain.Wiki2Plain(text)  #make the text to plain text\n",
    "        text = wiki2plain_instance.text\n",
    "        text = text.lower()  #convert all the text to lower case. Case folding\n",
    "        current_tokens = filter(None, re.split('\\W+', text))  #get the tokens now\n",
    "        current_tokens = [word for word in current_tokens if not word in self.stop_words]\n",
    "        token_freq_map = {}\n",
    "        for token in current_tokens:\n",
    "            token = self.cacheInStem(token)\n",
    "            if token not in token_freq_map:\n",
    "                token_freq_map[token] = 1.0\n",
    "            else:\n",
    "                token_freq_map[token] += 1.0\n",
    "        return token_freq_map\n",
    "\n",
    "    def getCategoryForEntity(self, wiki_entity):\n",
    "        site = pywikibot.Site('en', 'wikipedia')\n",
    "        page = pywikibot.Page(site, wiki_entity)\n",
    "        cat_values = page.categories()\n",
    "        cat_list = list(cat_values)\n",
    "        cat_names = []\n",
    "        for cat in cat_list:\n",
    "            if not cat.isHiddenCategory():\n",
    "                cat_names.append(cat.title())\n",
    "        return cat_names\n",
    "\n",
    "\n",
    "\n",
    "    def getEntityforCategory(self, category):\n",
    "        site = pywikibot.Site('en', 'wikipedia')\n",
    "        catdata = pywikibot.Category(site, title=category)\n",
    "        entities = catdata.articles()\n",
    "        return self.getRefinedEntity(entities)\n",
    "\n",
    "    def cacheInStem(self, token):\n",
    "        if token not in self.cache_stem:\n",
    "            self.cache_stem[token] = self.stemmer.stem(token)\n",
    "        return self.cache_stem[token]\n",
    "\n",
    "    def getRefinedEntity(self, entities):\n",
    "        refinedEntity = []\n",
    "        list_entities = list(entities)\n",
    "        if len(list_entities) <= self.k:\n",
    "            for entity in entities:\n",
    "                refinedEntity.append(entity.title())\n",
    "            return refinedEntity\n",
    "        else:\n",
    "            range_entity = range(0, len(list_entities))\n",
    "            list_sample = random.sample(range_entity, self.k)\n",
    "            for i in range(0, self.k):\n",
    "                refinedEntity.append(list_entities[list_sample[i]].title())\n",
    "            return refinedEntity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wiki2PlainText\n",
    "\n",
    "# from http://stackoverflow.com/questions/4460921/extract-the-first-paragraph-from-a-wikipedia-article-python\n",
    "\n",
    "import re\n",
    "\n",
    "class Wiki2Plain:\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "\n",
    "        self.text = wiki\n",
    "        self.text = self.unhtml(self.text)\n",
    "        self.text = self.unwiki(self.text)\n",
    "        self.text = self.punctuate(self.text)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "\n",
    "    def unwiki(self, wiki):\n",
    "        \"\"\"\n",
    "        Remove wiki markup from the text.\n",
    "        \"\"\"\n",
    "        wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
    "        wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r\"''+\", '', wiki)\n",
    "        wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
    "\n",
    "        return wiki\n",
    "\n",
    "    def unhtml(self, html):\n",
    "        \"\"\"\n",
    "        Remove HTML from the text.\n",
    "        \"\"\"\n",
    "        html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
    "        html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
    "        html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
    "        html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
    "        html = re.sub(r'(?m)<.*?>', '', html)\n",
    "        html = re.sub(r'(?i)&amp;', '&', html)\n",
    "\n",
    "        return html\n",
    "\n",
    "    def punctuate(self, text):\n",
    "        \"\"\"\n",
    "        Convert every text part into well-formed one-space\n",
    "        separate paragraph.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'\\r\\n|\\n|\\r', '\\n', text)\n",
    "        text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "\n",
    "        parts = text.split('\\n\\n')\n",
    "        partsParsed = []\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "\n",
    "            if len(part) == 0:\n",
    "                continue\n",
    "\n",
    "            partsParsed.append(part)\n",
    "\n",
    "        return '\\n\\n'.join(partsParsed)\n",
    "\n",
    "    def image(self):\n",
    "        \"\"\"\n",
    "        Retrieve the first image in the document.\n",
    "        \"\"\"\n",
    "        # match = re.search(r'(?i)\\|?\\s*(image|img|image_flag)\\s*=\\s*(<!--.*-->)?\\s*([^\\\\/:*?<>\"|%]+\\.[^\\\\/:*?<>\"|%]{3,4})', self.wiki)\n",
    "        match = re.search(r'(?i)([^\\\\/:*?<>\"|% =]+)\\.(gif|jpg|jpeg|png|bmp)', self.wiki)\n",
    "\n",
    "        if match:\n",
    "            return '%s.%s' % match.groups()\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivia Metric Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wiki-Trivia Metric Calculator\n",
    "# Since by default the encoding scheme is based on the operating system, we will enforce the encoding scheme to be utf-8\n",
    "\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import gensim\n",
    "import heapq\n",
    "import math\n",
    "import Util as util\n",
    "import pdb\n",
    "\n",
    "class WikiTriviaMetricCalculator:\n",
    "    def __init__(self):\n",
    "        print \"Inside the Initialization of the class: WikiTriviaExtractor\"\n",
    "        self.genism_model_filename = \"GoogleNews-vectors-negative300.bin\"\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(self.genism_model_filename, binary=True)\n",
    "        self.global_idf = util.getglobalfreqdict(\"plainIdfIndex.txt\")\n",
    "        self.k_val = 10\n",
    "        self.doc_size = 10000.0  #document size for the idf\n",
    "        self.rare_term_freq = 10  #used for ignoring rarely occuring terms.\n",
    "        print \"Initialization Done\"\n",
    "\n",
    "    def GetModel(self):\n",
    "        if self.model:\n",
    "            return\n",
    "        print 'Generating the Model'\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(self.genism_model_filename, binary=True)\n",
    "        print 'Model Generated'\n",
    "\n",
    "    # Get the top k tf idf tokens from the token freq map\n",
    "    def getTopKTFIDFforEntity(self, token_frequency):\n",
    "        entity_result = {}\n",
    "        for token in token_frequency:\n",
    "            if token not in self.model.vocab:\n",
    "                continue\n",
    "            tf = 1.0 + math.log10(token_frequency[token])\n",
    "            global_freq = self.global_idf[token] if token in self.global_idf else 1.5\n",
    "            if global_freq < self.rare_term_freq: #ignoring very rare terms\n",
    "                continue\n",
    "            entity_result[token] = tf * math.log10(self.doc_size/float(global_freq))\n",
    "        return heapq.nlargest(self.k_val, entity_result, key=entity_result.get)\n",
    "\n",
    "\n",
    "\n",
    "    def getEntitySimilarity(self,entity1, entity2):\n",
    "        sim1 = self.getEntitySimilarityHelper(entity1, entity2)\n",
    "        sim2 = self.getEntitySimilarityHelper(entity2, entity1)\n",
    "        return ((sim1 + sim2) / 2.0)\n",
    "\n",
    "    def getEntitySimilarityHelper(self, entity1, entity2):\n",
    "        sim = 0.0\n",
    "        if (len(entity1) < self.k_val or len(entity2) < self.k_val):\n",
    "            return 0.0\n",
    "        for i in range(0, self.k_val):\n",
    "            current_max = self.model.similarity(entity1[i], entity2[0])\n",
    "            for j in range(1, self.k_val):\n",
    "                current_val = self.model.similarity(entity1[i], entity2[j])\n",
    "                if current_val > current_max:\n",
    "                    current_max = current_val\n",
    "            sim += (self.k_val - i) * (current_max)\n",
    "        sim = sim / ((self.k_val+1.0) * (float(self.k_val)))\n",
    "        sim = sim * 2.0\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Algorithm Wrapper\n",
    "import wiki_parser\n",
    "import wiki_trivia_metric_calculator\n",
    "import Util as util\n",
    "import pdb\n",
    "import os\n",
    "import operator\n",
    "\n",
    "#globals\n",
    "category_entity_cache_dir = \"catentcache/\"\n",
    "output_cache_dir = \"outputCache/\"\n",
    "surprise_weight = 1.1\n",
    "\n",
    "def triviaAlgorithm(search_entity):\n",
    "    entity = util.searchWiki(search_entity)\n",
    "    print \"Entity Found\" + entity\n",
    "    # Check for the Output Cache\n",
    "    full_path = output_cache_dir + entity + \".txt\"\n",
    "    if not os.path.exists(output_cache_dir):\n",
    "        os.makedirs(output_cache_dir)\n",
    "\n",
    "    answer_mat = {}\n",
    "    if os.path.isfile(full_path):\n",
    "        open_output_file = open(full_path, \"r\")\n",
    "        for line in open_output_file:\n",
    "            trivia, score = line.split(\":\")\n",
    "            answer_mat[trivia] = score\n",
    "        open_output_file.close()\n",
    "        return answer_mat\n",
    "\n",
    "    wiki_parser_instance = wiki_parser.WikiParser()\n",
    "    wiki_trivia_metric_calculator_instance = wiki_trivia_metric_calculator.WikiTriviaMetricCalculator()\n",
    "    # If the cache doesn't exist- Make the new one for the said entity\n",
    "    entity_cats = wiki_parser_instance.getCategoryForEntity(entity)\n",
    "    if not entity_cats:\n",
    "        return\n",
    "    if not os.path.exists(category_entity_cache_dir):\n",
    "        os.makedirs(category_entity_cache_dir)\n",
    "    for entity_cat in entity_cats:\n",
    "        surprise_fact = surprise(entity, entity_cat, wiki_parser_instance, wiki_trivia_metric_calculator_instance)\n",
    "        if surprise_fact:\n",
    "            answer_mat[entity_cat.split(\":\")[1]] = surprise_fact\n",
    "            cohes_score = cohesivness(entity_cat.split(\":\")[1], wiki_trivia_metric_calculator_instance)\n",
    "            if cohes_score:\n",
    "                answer_mat[entity_cat.split(\":\")[1]] *= cohes_score\n",
    "            else:\n",
    "                answer_mat[entity_cat.split(\":\")[1]] = 0.0\n",
    "            print \"<------------- ----------------->\"\n",
    "            print \"Overall score for cat \", entity_cat, \" is \", answer_mat[entity_cat.split(\":\")[1]]\n",
    "            print \"Ending     <------------- ----------------->\"\n",
    "    answer_mat = sorted(answer_mat.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    target = open(full_path, \"w\")\n",
    "    for key in answer_mat:\n",
    "        target.write(key[0] + \":\" + repr(key[1]))\n",
    "        target.write(\"\\n\")\n",
    "    target.close()\n",
    "    return answer_mat\n",
    "\n",
    "def surprise(entity_input, entity_cat, wiki_parser_instance, wiki_trivia_metric_calculator_instance):\n",
    "    sum = 0.0\n",
    "    count = 0.0\n",
    "    entity_input_tokens = wiki_parser_instance.getEntityTokens(entity_input)\n",
    "    entity_input_top = wiki_trivia_metric_calculator_instance.getTopKTFIDFforEntity(entity_input_tokens)\n",
    "\n",
    "    path = category_entity_cache_dir + entity_cat.split(\":\")[1] + \"/\"\n",
    "    if os.path.exists(path):\n",
    "        print \"Reading from file \"\n",
    "        outer_list = []\n",
    "        for(root, dirs, files) in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    inner_list = []\n",
    "                    current_file = open(os.path.join(root, file), \"r\")\n",
    "                    for line in current_file:\n",
    "                        line = line.replace('\\n', '')\n",
    "                        inner_list.append(line)\n",
    "                    outer_list.append(inner_list)\n",
    "        size_new = len(outer_list)\n",
    "        for i in range(0, size_new):\n",
    "            sum += wiki_trivia_metric_calculator_instance.getEntitySimilarity(entity_input_top, outer_list[i])\n",
    "            count += 1.0\n",
    "        answer = sum / count\n",
    "        print \"surprise for \", entity_cat, \" is \", (1.0 / answer)\n",
    "        return (1.0 / answer)\n",
    "\n",
    "    new_entities = wiki_parser_instance.getEntityforCategory(entity_cat)\n",
    "    if not new_entities:\n",
    "        return\n",
    "\n",
    "    for new_entity in new_entities:\n",
    "        if new_entity != entity_input:\n",
    "            new_entity_tokens = wiki_parser_instance.getEntityTokens(new_entity)\n",
    "            new_entity_tokens_top = wiki_trivia_metric_calculator_instance.getTopKTFIDFforEntity(new_entity_tokens)\n",
    "            new_entity_top_cache = category_entity_cache_dir + entity_cat.split(':')[1] + \"/\"\n",
    "            if not os.path.exists(new_entity_top_cache):\n",
    "                os.makedirs(new_entity_top_cache)\n",
    "            cache_file_name = new_entity_top_cache + new_entity + \".txt\"\n",
    "            target = open(cache_file_name, \"w\")\n",
    "            for top_token in new_entity_tokens_top:\n",
    "                target.write(top_token)\n",
    "                target.write(\"\\n\")\n",
    "            target.close()\n",
    "            sum += wiki_trivia_metric_calculator_instance.getEntitySimilarity(entity_input_top, new_entity_tokens_top)\n",
    "            count += 1.0\n",
    "    answer = sum / count\n",
    "    print \"surprise for \" , entity_cat, \" is \", (1.0/answer)\n",
    "    return (1.0 / answer)\n",
    "\n",
    "def cohesivness(entity_cat, wiki_trivia_metric_calculator_instance):\n",
    "    sum = 0.0\n",
    "    count = 0.0\n",
    "    #answer = sum / count\n",
    "    path = category_entity_cache_dir + entity_cat + \"/\"\n",
    "    outer_list = []\n",
    "    for(root, dirs, files) in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                inner_list = []\n",
    "                current_file = open(os.path.join(root, file), \"r\")\n",
    "                for line in current_file:\n",
    "                    line = line.replace('\\n', '')\n",
    "                    inner_list.append(line)\n",
    "                outer_list.append(inner_list)\n",
    "    size_new = len(outer_list)\n",
    "    for i in range(0, size_new):\n",
    "        for j in range(i+1, size_new):\n",
    "                sum += wiki_trivia_metric_calculator_instance.getEntitySimilarity(outer_list[i], outer_list[j])\n",
    "                count += 1.0\n",
    "    if count == 0.0:\n",
    "        return 0.0\n",
    "    answer = sum / count\n",
    "    print \"Cohes is for cat \", entity_cat, \" is \", answer\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main calling Function Entry Points ( This heading will be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main Algorithm Calling Function\n",
    "import algorithm_wrapper\n",
    "import wikipedia as wiki\n",
    "import pdb\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        input_entity = raw_input()\n",
    "        if input_entity == \"1\":\n",
    "            break\n",
    "        print algorithm_wrapper.triviaAlgorithm(input_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Work\n",
    "\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
